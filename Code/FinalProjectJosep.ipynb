{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project\n",
    "\n",
    "Josep Puig Ruiz\n",
    "\n",
    "Due December 11th, 2019\n",
    "\n",
    "EN.553.688: Financial Computing, Fall 2019\n",
    "\n",
    "Instructor: Dr. Naiman\n",
    "\n",
    "----\n",
    "\n",
    "# Importing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import statistics\n",
    "from sklearn import tree\n",
    "from sklearn import svm\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading data from CSV\n",
    "dfA0 = pd.read_csv(\"A0.csv\").set_index(\"LID\")\n",
    "dfA1 = pd.read_csv(\"A1.csv\").set_index(\"LID\")\n",
    "dfA2 = pd.read_csv(\"A2.csv\").set_index(\"LID\")\n",
    "dfCF0 = pd.read_csv(\"CASH_FLOW0.csv\").set_index(\"LID\")\n",
    "dfCF1 = pd.read_csv(\"CASH_FLOW1.csv\").set_index(\"LID\")\n",
    "\n",
    "# Function to preprocess Acquisition DataFrames\n",
    "def preprocessdfA(dfA):\n",
    "    # Convert ORIG_DATE into days from \"2000-01-01\"\n",
    "    dfA[\"ORIG_DATE\"] = pd.to_datetime(dfA[\"ORIG_DATE\"]).apply(findDays)\n",
    "    \n",
    "    # Delete features that I do not consider for my model \n",
    "    del dfA['FIRST_PMT_DATE']\n",
    "    del dfA['MORT_INS_PCT']\n",
    "    del dfA['MORT_INS_TYPE']\n",
    "    del dfA['SELLER']\n",
    "    del dfA['ZIP']\n",
    "    del dfA['CO_CREDIT_SCORE']\n",
    "    del dfA['STATE']\n",
    "    del dfA['OCHANNEL']\n",
    "    del dfA['PROD_TYPE']\n",
    "    del dfA['OCC_STATUS']\n",
    "    \n",
    "    # Fill missing values \n",
    "    dfA['CLTV'].fillna((dfA['CLTV'].mean()), inplace=True)\n",
    "    dfA['LTV'].fillna((dfA['CLTV'].mean()), inplace=True)\n",
    "    dfA['NBORROWERS'].fillna((dfA['NBORROWERS'].mean()), inplace=True)\n",
    "    dfA['DTI'].fillna((dfA['DTI'].mean()), inplace=True)\n",
    "    dfA['CREDIT_SCORE'].fillna((dfA['CREDIT_SCORE'].mean()), inplace=True)\n",
    "    \n",
    "    # Encode categorical features\n",
    "    categorical_feature_mask = dfA.dtypes==object\n",
    "    categorical_cols = dfA.columns[categorical_feature_mask].tolist()\n",
    "    le = LabelEncoder()\n",
    "    dfA[categorical_cols] = dfA[categorical_cols].apply(lambda col: le.fit_transform(col))\n",
    "    \n",
    "    return\n",
    "\n",
    "# Function to find days from arbitrary date\n",
    "def findDays(a):\n",
    "    date_format = \"%Y-%d-%m\"\n",
    "    b = datetime.strptime(\"2000-01-01\", date_format)\n",
    "    return (a-b).days\n",
    "\n",
    "# Preprocess the Acquisition DataFrames\n",
    "preprocessdfA(dfA0)\n",
    "preprocessdfA(dfA1)\n",
    "preprocessdfA(dfA2)\n",
    "\n",
    "# Function to preprocess Performance DataFrames\n",
    "def preprocessCF(dfCF):    \n",
    "    Y = pd.pivot_table(dfCF, index = ['LID'], \n",
    "                            aggfunc={'LOAN_AGE':[p12, p24, p36, p48, p60], \n",
    "                                     'PAYMENT':[r12, r24, r36, r48, r60]})\n",
    "    return Y\n",
    "\n",
    "#Required functions to preprocess Performance DataFrames\n",
    "def p12(a):\n",
    "    return 1 if len(a)>=12 else 0\n",
    "def p24(a):\n",
    "    return 1 if len(a)>=24 else 0\n",
    "def p36(a):\n",
    "    return 1 if len(a)>=36 else 0\n",
    "def p48(a):\n",
    "    return 1 if len(a)>=48 else 0\n",
    "def p60(a):\n",
    "    return 1 if len(a)>=60 else 0\n",
    "def r12(a):\n",
    "    return sum([a.iloc[i] for i in range(min(13, len(a)))]) \n",
    "def r24(a):\n",
    "    return sum([a.iloc[i] for i in range(min(25, len(a)))])\n",
    "def r36(a):\n",
    "    return sum([a.iloc[i] for i in range(min(37, len(a)))])\n",
    "def r48(a):\n",
    "    return sum([a.iloc[i] for i in range(min(49, len(a)))])\n",
    "def r60(a):\n",
    "    return sum([a.iloc[i] for i in range(min(61, len(a)))])\n",
    "\n",
    "\n",
    "# Preprocess Performance Data Frames\n",
    "X0 = dfA0\n",
    "y0 = preprocessCF(dfCF0)\n",
    "X0.sort_index(ascending=True, inplace=True)\n",
    "y0.sort_index(ascending=True, inplace=True)\n",
    "\n",
    "X1 = dfA1\n",
    "y1 = preprocessCF(dfCF1)\n",
    "X1.sort_index(ascending=True, inplace=True)\n",
    "y1.sort_index(ascending=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "# Estimating loan active chance, p(m), from A0, CF0, A1, and CF1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ML Algorithm</th>\n",
       "      <th>score0p12</th>\n",
       "      <th>score1p12</th>\n",
       "      <th>score0p24</th>\n",
       "      <th>score1p24</th>\n",
       "      <th>score0p36</th>\n",
       "      <th>score1p36</th>\n",
       "      <th>score0p48</th>\n",
       "      <th>score1p48</th>\n",
       "      <th>score0p60</th>\n",
       "      <th>score1p60</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.145754</td>\n",
       "      <td>0.146201</td>\n",
       "      <td>0.392712</td>\n",
       "      <td>0.397535</td>\n",
       "      <td>0.218472</td>\n",
       "      <td>0.219845</td>\n",
       "      <td>0.141028</td>\n",
       "      <td>0.140676</td>\n",
       "      <td>0.101491</td>\n",
       "      <td>0.101779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Logistic Regression with Prob</td>\n",
       "      <td>0.244411</td>\n",
       "      <td>0.245538</td>\n",
       "      <td>0.468857</td>\n",
       "      <td>0.470413</td>\n",
       "      <td>0.315654</td>\n",
       "      <td>0.317952</td>\n",
       "      <td>0.223259</td>\n",
       "      <td>0.224152</td>\n",
       "      <td>0.169683</td>\n",
       "      <td>0.170887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.159455</td>\n",
       "      <td>0.160700</td>\n",
       "      <td>0.378629</td>\n",
       "      <td>0.377224</td>\n",
       "      <td>0.222208</td>\n",
       "      <td>0.223454</td>\n",
       "      <td>0.146457</td>\n",
       "      <td>0.146329</td>\n",
       "      <td>0.105068</td>\n",
       "      <td>0.105930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.149970</td>\n",
       "      <td>0.151598</td>\n",
       "      <td>0.371060</td>\n",
       "      <td>0.372018</td>\n",
       "      <td>0.226519</td>\n",
       "      <td>0.228020</td>\n",
       "      <td>0.148085</td>\n",
       "      <td>0.147894</td>\n",
       "      <td>0.105835</td>\n",
       "      <td>0.107559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Gaussian NB</td>\n",
       "      <td>0.157666</td>\n",
       "      <td>0.161307</td>\n",
       "      <td>0.441574</td>\n",
       "      <td>0.446109</td>\n",
       "      <td>0.230256</td>\n",
       "      <td>0.234982</td>\n",
       "      <td>0.162840</td>\n",
       "      <td>0.164436</td>\n",
       "      <td>0.129595</td>\n",
       "      <td>0.132118</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    ML Algorithm  score0p12  score1p12  score0p24  score1p24  \\\n",
       "0            Logistic Regression   0.145754   0.146201   0.392712   0.397535   \n",
       "1  Logistic Regression with Prob   0.244411   0.245538   0.468857   0.470413   \n",
       "2                  Random Forest   0.159455   0.160700   0.378629   0.377224   \n",
       "3                  Decision Tree   0.149970   0.151598   0.371060   0.372018   \n",
       "4                    Gaussian NB   0.157666   0.161307   0.441574   0.446109   \n",
       "\n",
       "   score0p36  score1p36  score0p48  score1p48  score0p60  score1p60  \n",
       "0   0.218472   0.219845   0.141028   0.140676   0.101491   0.101779  \n",
       "1   0.315654   0.317952   0.223259   0.224152   0.169683   0.170887  \n",
       "2   0.222208   0.223454   0.146457   0.146329   0.105068   0.105930  \n",
       "3   0.226519   0.228020   0.148085   0.147894   0.105835   0.107559  \n",
       "4   0.230256   0.234982   0.162840   0.164436   0.129595   0.132118  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function to find a score in case y_predicted is a 0 or a 1\n",
    "def score(y_real, y_predicted):\n",
    "    return (sum([1 for i in range(len(y_real)) if y_real[i] != y_predicted[i] ])/len(y_real))\n",
    "\n",
    "# Function to find a score in case y_predicted is a continuous value between 0 and 1\n",
    "def scoreProb(y_real, y_predicted):\n",
    "    return statistics.mean([abs(y_real[i]-y_predicted[i][1]) for i in range(len(y_real))])\n",
    "    \n",
    "# Function to find scores to predict algorithm performance\n",
    "def getScores(dataSetNumber, pX):\n",
    "    if dataSetNumber == 0:\n",
    "        X, y = X0, y0\n",
    "    elif dataSetNumber == 1:\n",
    "        X, y = X1, y1\n",
    "    else: \n",
    "        print(\"dataSetNumber can be only 0 or 1. Error\")\n",
    "        return\n",
    "    # Split data into Train & Test\n",
    "    Xtrain, Xtest, ytrain, ytest = train_test_split(X.values, y.loc[:,(\"LOAN_AGE\", pX)].values, \n",
    "                                                    test_size=0.2, random_state=0)\n",
    "    \n",
    "    \n",
    "    # Logistic Regression    \n",
    "    logmodel = LogisticRegression(solver=\"lbfgs\")\n",
    "    logmodel.fit(Xtrain, ytrain)\n",
    "    ypred = logmodel.predict(Xtest)\n",
    "    sLR = score(ytest, ypred)\n",
    "    \n",
    "    # Logistic Regression with probability    \n",
    "    ypred = logmodel.predict_proba(Xtest)\n",
    "    sLRp = scoreProb(ytest, ypred)\n",
    "    \n",
    "    # Random Forest\n",
    "    rfclass = RandomForestClassifier(n_estimators = 10, random_state = 42)\n",
    "    rfclass.fit(Xtrain, ytrain)\n",
    "    ypred = rfclass.predict(Xtest)\n",
    "    sRF = score(ytest, ypred)\n",
    "    \n",
    "    # Decision Tree\n",
    "    treeclass = tree.DecisionTreeClassifier(min_samples_split=25,min_samples_leaf=25)\n",
    "    treeclass = treeclass.fit(Xtrain, ytrain)\n",
    "    ypred = treeclass.predict(Xtest)\n",
    "    sDT = score(ytest, ypred)\n",
    "\n",
    "    # Gaussian NB\n",
    "    gnb = GaussianNB()\n",
    "    gnbclass = gnb.fit(Xtrain, ytrain)\n",
    "    ypred = gnbclass.predict(Xtest)\n",
    "    sGNB = score(ytest, ypred)\n",
    "    \n",
    "    return [sLR, sLRp, sRF, sDT, sGNB]\n",
    "\n",
    "\n",
    "# Create a dataframe to store results\n",
    "scores = {'ML Algorithm':['Logistic Regression', 'Logistic Regression with Prob','Random Forest', \n",
    "                          'Decision Tree', 'Gaussian NB']}\n",
    "dfP = pd.DataFrame(scores) \n",
    "\n",
    "pXlist = [\"p60\", \"p48\", \"p36\", \"p24\", \"p12\"]\n",
    "\n",
    "# Double loop to use find scores for algorithms, using both datasets 0 and 1, and also 12, 24, ..., 60 months\n",
    "for pX in pXlist:\n",
    "    for i in [1, 0]:\n",
    "        nameColumn = \"score\"+str(i)+pX\n",
    "        dfP.insert(1, nameColumn, getScores(i, pX))\n",
    "\n",
    "#\n",
    "dfP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "# Estimating cumulative revenues, r(m), from A0, CF0, A1, and CF1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ML Algorithm</th>\n",
       "      <th>error0r12</th>\n",
       "      <th>error1r12</th>\n",
       "      <th>error0r24</th>\n",
       "      <th>error1r24</th>\n",
       "      <th>error0r36</th>\n",
       "      <th>error1r36</th>\n",
       "      <th>error0r48</th>\n",
       "      <th>error1r48</th>\n",
       "      <th>error0r60</th>\n",
       "      <th>error1r60</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>No Polyn Feat</td>\n",
       "      <td>45146.482315</td>\n",
       "      <td>44921.199498</td>\n",
       "      <td>52235.489734</td>\n",
       "      <td>52072.389364</td>\n",
       "      <td>26949.938751</td>\n",
       "      <td>27006.449521</td>\n",
       "      <td>17574.723187</td>\n",
       "      <td>17597.281899</td>\n",
       "      <td>13260.524131</td>\n",
       "      <td>13300.445677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Polyn Feat=2</td>\n",
       "      <td>41309.782589</td>\n",
       "      <td>40984.623562</td>\n",
       "      <td>50790.067303</td>\n",
       "      <td>50784.255857</td>\n",
       "      <td>26570.419453</td>\n",
       "      <td>26555.996499</td>\n",
       "      <td>17290.061858</td>\n",
       "      <td>17282.126608</td>\n",
       "      <td>13070.907878</td>\n",
       "      <td>13103.745736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Polyn Feat=2, interactions only</td>\n",
       "      <td>41629.786910</td>\n",
       "      <td>41378.536269</td>\n",
       "      <td>51141.600388</td>\n",
       "      <td>51116.554357</td>\n",
       "      <td>26972.102978</td>\n",
       "      <td>27000.679034</td>\n",
       "      <td>17470.170961</td>\n",
       "      <td>17489.102394</td>\n",
       "      <td>13123.272003</td>\n",
       "      <td>13167.677025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>40642.218351</td>\n",
       "      <td>40362.870625</td>\n",
       "      <td>50323.844590</td>\n",
       "      <td>50249.051099</td>\n",
       "      <td>26477.067487</td>\n",
       "      <td>26481.922653</td>\n",
       "      <td>17322.874810</td>\n",
       "      <td>17324.209036</td>\n",
       "      <td>13101.987670</td>\n",
       "      <td>13163.846933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>40782.356901</td>\n",
       "      <td>40528.019144</td>\n",
       "      <td>50759.435038</td>\n",
       "      <td>50890.275622</td>\n",
       "      <td>26725.777727</td>\n",
       "      <td>26694.735586</td>\n",
       "      <td>17349.238495</td>\n",
       "      <td>17404.920785</td>\n",
       "      <td>13097.156791</td>\n",
       "      <td>13205.929485</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      ML Algorithm     error0r12     error1r12     error0r24  \\\n",
       "0                    No Polyn Feat  45146.482315  44921.199498  52235.489734   \n",
       "1                     Polyn Feat=2  41309.782589  40984.623562  50790.067303   \n",
       "2  Polyn Feat=2, interactions only  41629.786910  41378.536269  51141.600388   \n",
       "3                                3  40642.218351  40362.870625  50323.844590   \n",
       "4                                4  40782.356901  40528.019144  50759.435038   \n",
       "\n",
       "      error1r24     error0r36     error1r36     error0r48     error1r48  \\\n",
       "0  52072.389364  26949.938751  27006.449521  17574.723187  17597.281899   \n",
       "1  50784.255857  26570.419453  26555.996499  17290.061858  17282.126608   \n",
       "2  51116.554357  26972.102978  27000.679034  17470.170961  17489.102394   \n",
       "3  50249.051099  26477.067487  26481.922653  17322.874810  17324.209036   \n",
       "4  50890.275622  26725.777727  26694.735586  17349.238495  17404.920785   \n",
       "\n",
       "      error0r60     error1r60  \n",
       "0  13260.524131  13300.445677  \n",
       "1  13070.907878  13103.745736  \n",
       "2  13123.272003  13167.677025  \n",
       "3  13101.987670  13163.846933  \n",
       "4  13097.156791  13205.929485  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function to find a score in case y_predicted is a 0 or a 1\n",
    "def score(y_real, y_predicted):\n",
    "    return statistics.mean([abs(y_real[i]-y_predicted[i]) for i in range(len(y_real))])\n",
    "\n",
    "# Function to find scores to predict algorithm performance\n",
    "def getScores(dataSetNumber, rX):\n",
    "    if dataSetNumber == 0:\n",
    "        X, y = X0, y0\n",
    "    elif dataSetNumber == 1:\n",
    "        X, y = X1, y1\n",
    "    else: \n",
    "        print(\"dataSetNumber can be only 0 or 1. Error\")\n",
    "        return\n",
    "    \n",
    "    # Split data into Train & Test\n",
    "    Xtrain, Xtest, ytrain, ytest = train_test_split(X.values, y.loc[:,(\"PAYMENT\", rX)].values, \n",
    "                                                    test_size=0.2, random_state=0)\n",
    "    \n",
    "    \n",
    "    # Linear Regression without polynomial feature\n",
    "    reg = LinearRegression().fit(Xtrain, ytrain)\n",
    "    ypred = reg.predict(Xtest)\n",
    "    lr0 = score(ytest, ypred)\n",
    "    \n",
    "    # Linear Regression without polynomial feature\n",
    "    poly = PolynomialFeatures(2, interaction_only = False)\n",
    "    X2=poly.fit_transform(Xtrain)\n",
    "    Xtest2 = poly.transform(Xtest)\n",
    "    reg = LinearRegression().fit(X2, ytrain)\n",
    "    ypred = reg.predict(Xtest2)\n",
    "    lr1 = score(ytest, ypred)\n",
    "    \n",
    "    # Linear Regression with polynomial feature, only interaction\n",
    "    poly = PolynomialFeatures(2, interaction_only = True)\n",
    "    X3=poly.fit_transform(Xtrain)\n",
    "    Xtest3 = poly.transform(Xtest)\n",
    "    reg = LinearRegression().fit(X3, ytrain)\n",
    "    ypred = reg.predict(Xtest3)\n",
    "    lr2 = score(ytest, ypred)\n",
    "\n",
    "    # Linear Regression with polynomial feature, degree 3\n",
    "    poly = PolynomialFeatures(3, interaction_only = False)\n",
    "    X4=poly.fit_transform(Xtrain)\n",
    "    Xtest4 = poly.transform(Xtest)\n",
    "    reg = LinearRegression().fit(X4, ytrain)\n",
    "    ypred = reg.predict(Xtest4)\n",
    "    lr3 = score(ytest, ypred)\n",
    "\n",
    "    # Linear Regression with polynomial feature, degree 4\n",
    "    poly = PolynomialFeatures(4, interaction_only = False)\n",
    "    X4=poly.fit_transform(Xtrain)\n",
    "    Xtest4 = poly.transform(Xtest)\n",
    "    reg = LinearRegression().fit(X4, ytrain)\n",
    "    ypred = reg.predict(Xtest4)\n",
    "    lr4 = score(ytest, ypred)\n",
    "\n",
    "    \n",
    "    return [lr0, lr1, lr2, lr3, lr4]\n",
    "\n",
    "# Create a dataframe to store results\n",
    "scores = {'ML Algorithm':['No Polyn Feat', 'Polyn Feat=2', 'Polyn Feat=2, interactions only', 'Polyn Feat=3', 'Polyn Feat=4']}\n",
    "dfR = pd.DataFrame(scores) \n",
    "\n",
    "\n",
    "# Double loop to use find scores for algorithms, using both datasets 0 and 1, and also 12, 24, ..., 60 months\n",
    "rXlist = [\"r60\", \"r48\", \"r36\", \"r24\", \"r12\"]\n",
    "for rX in rXlist:\n",
    "    for i in [1, 0]:\n",
    "        nameColumn = \"error\"+str(i)+rX\n",
    "        dfR.insert(1, nameColumn, getScores(i, rX))\n",
    "\n",
    "dfR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "\n",
    "#Â Predict performance for A2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate data from datasets 0 and 1 to use it all for training\n",
    "Xtrain = pd.concat([X0,X1], axis=0)\n",
    "ytrain = pd.concat([y0,y1], axis=0)\n",
    "Xtrain.sort_index(ascending=True, inplace=True)\n",
    "ytrain.sort_index(ascending=True, inplace=True)\n",
    "\n",
    "# dfA2 stores the acquisition data for dataset 2\n",
    "X2 = dfA2\n",
    "\n",
    "\n",
    "# Apply the best performing algorithms to calculate p12, p24, ..., p60\n",
    "\n",
    "logmodel = LogisticRegression(solver=\"lbfgs\")\n",
    "logmodel.fit(Xtrain.values, ytrain.loc[:,(\"LOAN_AGE\", \"p12\")].values)\n",
    "p12 = logmodel.predict(X2)\n",
    "\n",
    "treeclass = tree.DecisionTreeClassifier(min_samples_split=25,min_samples_leaf=25)\n",
    "treeclass = treeclass.fit(Xtrain, ytrain.loc[:,(\"LOAN_AGE\", \"p24\")].values)\n",
    "p24 = treeclass.predict(X2)\n",
    "\n",
    "logmodel = LogisticRegression(solver=\"lbfgs\")\n",
    "logmodel.fit(Xtrain.values, ytrain.loc[:,(\"LOAN_AGE\", \"p36\")].values)\n",
    "p36 = logmodel.predict(X2)\n",
    "\n",
    "logmodel = LogisticRegression(solver=\"lbfgs\")\n",
    "logmodel.fit(Xtrain.values, ytrain.loc[:,(\"LOAN_AGE\", \"p48\")].values)\n",
    "p48 = logmodel.predict(X2)\n",
    "\n",
    "logmodel = LogisticRegression(solver=\"lbfgs\")\n",
    "logmodel.fit(Xtrain.values, ytrain.loc[:,(\"LOAN_AGE\", \"p60\")].values)\n",
    "p60 = logmodel.predict(X2)\n",
    "\n",
    "\n",
    "# Apply the best performing algorithms to calculate r12, r24, ..., r60\n",
    "\n",
    "poly = PolynomialFeatures(3, interaction_only = False)\n",
    "XtrainPF=poly.fit_transform(Xtrain)\n",
    "X2PF = poly.transform(X2)\n",
    "reg = LinearRegression().fit(XtrainPF, ytrain.loc[:,(\"PAYMENT\", \"r12\")].values)\n",
    "r12 = reg.predict(X2PF)\n",
    "\n",
    "poly = PolynomialFeatures(3, interaction_only = False)\n",
    "XtrainPF=poly.fit_transform(Xtrain)\n",
    "X2PF = poly.transform(X2)\n",
    "reg = LinearRegression().fit(XtrainPF, ytrain.loc[:,(\"PAYMENT\", \"r12\")].values)\n",
    "r24 = reg.predict(X2PF)\n",
    "\n",
    "poly = PolynomialFeatures(3, interaction_only = False)\n",
    "XtrainPF=poly.fit_transform(Xtrain)\n",
    "X2PF = poly.transform(X2)\n",
    "reg = LinearRegression().fit(XtrainPF, ytrain.loc[:,(\"PAYMENT\", \"r12\")].values)\n",
    "r36 = reg.predict(X2PF)\n",
    "\n",
    "poly = PolynomialFeatures(2, interaction_only = False)\n",
    "XtrainPF=poly.fit_transform(Xtrain)\n",
    "X2PF = poly.transform(X2)\n",
    "reg = LinearRegression().fit(XtrainPF, ytrain.loc[:,(\"PAYMENT\", \"r12\")].values)\n",
    "r48 = reg.predict(X2PF)\n",
    "\n",
    "poly = PolynomialFeatures(2, interaction_only = False)\n",
    "XtrainPF=poly.fit_transform(Xtrain)\n",
    "X2PF = poly.transform(X2)\n",
    "reg = LinearRegression().fit(XtrainPF, ytrain.loc[:,(\"PAYMENT\", \"r12\")].values)\n",
    "r60 = reg.predict(X2PF)\n",
    "\n",
    "\n",
    "# Save the predicted values into a DataFrame\n",
    "dfOutput = pd.DataFrame(index=X2.index)\n",
    "dfOutput.insert(0, \"r(60)\", r60)\n",
    "dfOutput.insert(0, \"r(48)\", r48)\n",
    "dfOutput.insert(0, \"r(36)\", r36)\n",
    "dfOutput.insert(0, \"r(24)\", r24)\n",
    "dfOutput.insert(0, \"r(12)\", r12)\n",
    "dfOutput.insert(0, \"p(60)\", p60)\n",
    "dfOutput.insert(0, \"p(48)\", p48)\n",
    "dfOutput.insert(0, \"p(36)\", p36)\n",
    "dfOutput.insert(0, \"p(24)\", p24)\n",
    "dfOutput.insert(0, \"p(12)\", p12)\n",
    "\n",
    "# Export predicted values into csv file\n",
    "dfOutput.to_csv(r'predictions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>p(12)</th>\n",
       "      <th>p(24)</th>\n",
       "      <th>p(36)</th>\n",
       "      <th>p(48)</th>\n",
       "      <th>p(60)</th>\n",
       "      <th>r(12)</th>\n",
       "      <th>r(24)</th>\n",
       "      <th>r(36)</th>\n",
       "      <th>r(48)</th>\n",
       "      <th>r(60)</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>899993094470</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19220.285672</td>\n",
       "      <td>19220.285672</td>\n",
       "      <td>19220.285672</td>\n",
       "      <td>34832.315102</td>\n",
       "      <td>34832.315102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899984345565</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20298.503803</td>\n",
       "      <td>20298.503803</td>\n",
       "      <td>20298.503803</td>\n",
       "      <td>36344.393849</td>\n",
       "      <td>36344.393849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899962467067</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>66227.309269</td>\n",
       "      <td>66227.309269</td>\n",
       "      <td>66227.309269</td>\n",
       "      <td>92823.555975</td>\n",
       "      <td>92823.555975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899956562768</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>41100.489221</td>\n",
       "      <td>41100.489221</td>\n",
       "      <td>41100.489221</td>\n",
       "      <td>89393.455914</td>\n",
       "      <td>89393.455914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899955704832</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21953.316045</td>\n",
       "      <td>21953.316045</td>\n",
       "      <td>21953.316045</td>\n",
       "      <td>41179.306050</td>\n",
       "      <td>41179.306050</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              p(12)  p(24)  p(36)  p(48)  p(60)         r(12)         r(24)  \\\n",
       "LID                                                                           \n",
       "899993094470    1.0    1.0    0.0    0.0    0.0  19220.285672  19220.285672   \n",
       "899984345565    1.0    0.0    0.0    0.0    0.0  20298.503803  20298.503803   \n",
       "899962467067    1.0    0.0    0.0    0.0    0.0  66227.309269  66227.309269   \n",
       "899956562768    1.0    0.0    0.0    0.0    0.0  41100.489221  41100.489221   \n",
       "899955704832    1.0    1.0    0.0    0.0    0.0  21953.316045  21953.316045   \n",
       "\n",
       "                     r(36)         r(48)         r(60)  \n",
       "LID                                                     \n",
       "899993094470  19220.285672  34832.315102  34832.315102  \n",
       "899984345565  20298.503803  36344.393849  36344.393849  \n",
       "899962467067  66227.309269  92823.555975  92823.555975  \n",
       "899956562768  41100.489221  89393.455914  89393.455914  \n",
       "899955704832  21953.316045  41179.306050  41179.306050  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the predicted values into a DataFrame\n",
    "dfOutput = pd.DataFrame(index=X2.index)\n",
    "dfOutput.insert(0, \"r(60)\", r60)\n",
    "dfOutput.insert(0, \"r(48)\", r48)\n",
    "dfOutput.insert(0, \"r(36)\", r36)\n",
    "dfOutput.insert(0, \"r(24)\", r24)\n",
    "dfOutput.insert(0, \"r(12)\", r12)\n",
    "dfOutput.insert(0, \"p(60)\", p60)\n",
    "dfOutput.insert(0, \"p(48)\", p48)\n",
    "dfOutput.insert(0, \"p(36)\", p36)\n",
    "dfOutput.insert(0, \"p(24)\", p24)\n",
    "dfOutput.insert(0, \"p(12)\", p12)\n",
    "\n",
    "# Export predicted values into csv file\n",
    "dfOutput.to_csv(r'predictions.csv')\n",
    "\n",
    "dfOutput.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "# END OF PROJECT\n",
    "\n",
    "The remaining code snippets are from alternative approaches that I tried."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Decision Tree\n",
    "treeclass = tree.DecisionTreeClassifier(min_samples_split=25,min_samples_leaf=25)\n",
    "treeclass = treeclass.fit(X_train, y_train)\n",
    "y_tree = treeclass.predict(X_test)\n",
    "CFtree = confusion_matrix(y_test, y_tree)\n",
    "print(\"Confusion Matrix for DECISION TREE: \\n\", CFtree)\n",
    "print(\"Accuracy: \", accuracy_CF(CFtree))\n",
    "\n",
    "# Gaussian NB\n",
    "gnb = GaussianNB()\n",
    "gnbclass = gnb.fit(X_train, y_train)\n",
    "y_gnb = gnbclass.predict(X_test)\n",
    "CFgnb = confusion_matrix(y_test, y_gnb)\n",
    "print(\"Confusion Matrix for Gaussian Naive Bayes: \\n\", CFgnb)\n",
    "print(\"Accuracy: \", accuracy_CF(CFgnb))\n",
    "\n",
    "# Neural Network\n",
    "nnwclass = MLPClassifier().fit(X_train, y_train)\n",
    "y_nnw = nnwclass.predict(X_test)\n",
    "CFnnw = confusion_matrix(y_test, y_nnw)\n",
    "print(\"Confusion Matrix for NEURAL NETWORK: \\n\", CFnnw)\n",
    "print(\"Accuracy: \", accuracy_CF(CFnnw))\n",
    "\n",
    "# SVM\n",
    "svmclass = svm.SVC()\n",
    "svmclass.fit(X_train, y_train)\n",
    "y_svm = svmclass.predict(X_test)\n",
    "CFsvm = confusion_matrix(y_test, y_svm)\n",
    "print(\"Confusion Matrix for SVM: \\n\", CFsvm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(Xtrain0.values, Ytrain0.loc[:,(\"LOAN_AGE\", \"p60\")].values, \n",
    "                                                    test_size=0.25, random_state=0)\n",
    "avg_score = [-1]*100\n",
    "\n",
    "\n",
    "# Logistic Regession\n",
    "logmodel = LogisticRegression()\n",
    "logmodel.fit(X_train, y_train)\n",
    "y_log_prob = logmodel.predict_proba(X_test)\n",
    "y_log = logmodel.predict(X_test)\n",
    "\n",
    "y_log_method= classifyLogistic(y_log_prob, 0.5)\n",
    "\n",
    "\n",
    "CFlog = confusion_matrix(y_test, y_log_method)\n",
    "print(\"Confusion Matrix for LOGISTIC REGRESSION: \\n\", CFlog)\n",
    "print(\"Accuracy: \", accuracy_CF(CFlog))\n",
    "\n",
    "\n",
    "\n",
    "X_score = Xtrain1\n",
    "y_score = Ytrain1.loc[:,(\"LOAN_AGE\", \"p60\")].values\n",
    "y_log_proba_score = logmodel.predict_proba(X_score)\n",
    "\n",
    "import statistics    \n",
    "def findScore(y_score, y_log_proba_score, threshold):\n",
    "    y_log = classifyLogistic(y_log_proba_score, threshold)\n",
    "    absDifference = statistics.mean([abs(y_score[i]-y_log[i]) for i in range(len(y_score))])\n",
    "    return absDifference\n",
    "\n",
    "\n",
    "def classifyLogistic(y_log_prob, threshold):\n",
    "    y_log = []\n",
    "    for [a,b] in y_log_prob:\n",
    "        if a<threshold:\n",
    "            y_log.append(1)\n",
    "        else:\n",
    "            y_log.append(0)\n",
    "    return y_log\n",
    "\n",
    "score = []\n",
    "threshold = [0.01+0.01*i for i in range(99)]\n",
    "for t in threshold:\n",
    "    print(\"Threshold: \", t, \"Mean of deviation: \", findScore(y_score, y_log_proba_score, t))\n",
    "    score.append(findScore(y_score, y_log_proba_score, t))\n",
    "\n",
    "\n",
    "plt.plot(threshold, score)\n",
    "print(y_log_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(Xtrain0.values, Ytrain0.loc[:,(\"LOAN_AGE\", \"p12\")].values, \n",
    "                                                    test_size=0.25, random_state=0)\n",
    "\n",
    "\n",
    "def accuracy_CF(CF):\n",
    "    return (CF[0][0]+CF[1][1])/(CF[0][0]+CF[1][1]+CF[1][0]+CF[0][1])\n",
    "\n",
    "def meanDifference(y_prob, y_real):\n",
    "    return(statistics.mean([abs(y_prob[i][1]-y_real[i]) for i in range(len(y_prob))]))\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "# Logistic Regession\n",
    "logmodel = LogisticRegression()\n",
    "logmodel.fit(X_train, y_train)\n",
    "y_log_prob = logmodel.predict_proba(X_test)\n",
    "y_log = logmodel.predict(X_test)\n",
    "CFlog = confusion_matrix(y_test, y_log)\n",
    "print(\"Confusion Matrix for LOGISTIC REGRESSION: \\n\", CFlog)\n",
    "print(\"Accuracy: \", accuracy_CF(CFlog))\n",
    "print(\"Mean difference: \", meanDifference(y_log_prob, y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
